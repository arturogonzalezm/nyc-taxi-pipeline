[![codecov](https://codecov.io/gh/arturogonzalezm/nyc-taxi-pipeline/graph/badge.svg?token=4jNHztzVjc)](https://codecov.io/gh/arturogonzalezm/nyc-taxi-pipeline)

# NYC Taxi Data Pipeline

A **production-grade data pipeline** for processing NYC Taxi data with:
- â˜ï¸ **GCP Infrastructure**: BigQuery, Cloud Storage, VPC, IAM
- ğŸ  **Local Development**: Full pipeline runs locally with Docker before GCP deployment
- ğŸ’° **Cost-Optimized**: Uses GCP Free Tier, easy teardown when not in use
- ğŸ”’ **Production Security**: VPC, service accounts, least-privilege IAM, private IPs
- ğŸš€ **CI/CD Ready**: Automated testing, linting, Terraform validation

## ğŸ¯ Dual-Mode Architecture

This pipeline supports **TWO execution modes**:

### 1. ğŸ  Local Mode (Development & Testing)
- Runs completely on your machine
- Uses Docker Compose with MinIO and PostgreSQL
- **No GCP costs** - test everything before deployment
- **Command**: `make local-setup && make local-run`

### 2. â˜ï¸ GCP Mode (Production)
- Deploys to Google Cloud Platform
- Uses BigQuery, GCS
- Production VPC with private networking
- Full IAM security and service accounts
- **Command**: `make gcp-deploy && make gcp-run`

---

## ğŸ“‹ Table of Contents

- [Quick Start (Local)](#-quick-start-local-mode)
- [GCP Deployment](#-gcp-deployment-production-mode)
- [Architecture](#-architecture)
- [Data Model](#-data-model)
- [Cost Management](#-cost-management--free-tier)
- [Security](#-security-implementation)
- [Development Workflow](#-development-workflow)
- [Teardown & Cleanup](#-teardown--cleanup)

---

## ğŸš€ Quick Start (Local Mode)

Test the entire pipeline on your machine in **under 10 minutes**:

### Prerequisites
- Docker Desktop (8GB RAM minimum)
- Python 3.10+
- 10GB free disk space
- **No GCP account needed for local mode**

### Step 1: Clone and Setup

```bash
git clone https://github.com/arturogonzalezm/nyc-taxi-pipeline
cd nyc-taxi-pipeline

# Copy environment file
cp .env.example .env

# Install dependencies
make local-setup
```

### Step 2: Start Local Services

```bash
# Start MinIO and all services
make local-up

# Check services are running
docker-compose ps
```

You should see:
- PostgreSQL (port 5432)
- MinIO API (http://localhost:9000)
- MinIO Console (http://localhost:9001)

### Step 3: Run the Pipeline

```bash

# Option B: Run via Make (simpler for testing)
make local-run MONTHS=2023-01

# This runs:
# - Bronze bronze (downloads NYC data)
# - Gold gold (creates dimensional model)  
# - PostgreSQL loading (loads star schema)
```

### Step 4: Query the Data

```bash
# Connect to PostgreSQL
make local-db

# Run sample queries
SELECT COUNT(*) FROM fact_trips;
SELECT * FROM dim_location LIMIT 10;

# Or use pre-built queries
\i /sql/sample_queries.sql
```

### Step 5: Run Tests

```bash
# Run all tests locally
make test

# Run with coverage report
make test-coverage
```

### Step 6: Stop Services

```bash
# Stop all services (keeps data)
make local-down

# Clean everything (removes data)
make local-clean
```

---

## â˜ï¸ GCP Deployment (Production Mode)

Deploy to Google Cloud with **production-grade security and networking**.

### Prerequisites

1. **GCP Account**
   - New account gets $300 free credits (90 days)
   - Enable Billing (won't charge without explicit upgrade)
   - Free tier includes: 10GB BigQuery storage, 1TB queries/month

2. **Required Tools**
   ```bash
   # Install gcloud CLI
   curl https://sdk.cloud.google.com | bash
   exec -l $SHELL
   
   # Install Terraform
   brew install terraform  # Mac
   # or download from https://terraform.io
   
   # Authenticate
   gcloud auth login
   gcloud auth application-default login
   ```

### Step 1: Configure GCP Project

```bash
# Set your project ID
export GCP_PROJECT_ID="nyc-taxi-pipeline-485713"
export GCP_REGION="us-central1"  # Free tier eligible

# Create project (if new)
gcloud projects create $GCP_PROJECT_ID
gcloud config set project $GCP_PROJECT_ID

# Enable billing
# Go to: https://console.cloud.google.com/billing

# Verify setup
make gcp-verify
```

### Step 2: Configure Terraform

```bash
cd terraform

# Create your variables file
cat > terraform.tfvars << TFVARS
project_id  = "$GCP_PROJECT_ID"
region      = "us-central1"
environment = "prod"

# Enable production features
enable_bigquery      = true
enable_cloud_composer = true
enable_vpc          = true
enable_private_ip   = true

# Cost optimization
composer_node_count = 3  # Minimum for production
composer_machine_type = "n1-standard-1"  # Smallest

# Data retention (days)
bucket_lifecycle_rules = {
  bronze_archive_days = 90
  silver_archive_days = 180
  gold_archive_days   = 365
}
TFVARS
```

### Step 3: Deploy Infrastructure

```bash
# Initialize Terraform
make gcp-init

# Review what will be created (no charges yet)
make gcp-plan

# Deploy infrastructure
make gcp-deploy

# This creates:
# âœ“ VPC with private networking
# âœ“ GCS buckets (bronze, silver, gold)
# âœ“ BigQuery dataset with tables
# âœ“ Service accounts with IAM roles
# âœ“ Cloud Storage buckets
# âœ“ Firewall rules
# âœ“ NAT gateway for private instances
```

**Expected deployment time**: 20-30 minutes (mostly Cloud Composer setup)

```

### Step 5: Run Pipeline on GCP

```bash

# Option B: Trigger via CLI
make gcp-trigger-pipeline MONTHS=2023-01,2023-02,2023-03

# Monitor progress
make gcp-logs
```

### Step 6: Query BigQuery

```bash
# Via CLI
bq query --use_legacy_sql=false '
SELECT 
    d.year,
    d.month,
    COUNT(*) as total_trips,
    SUM(f.total_amount) as total_revenue
FROM `'$GCP_PROJECT_ID'.nyc_taxi_warehouse.fact_trips` f
JOIN `'$GCP_PROJECT_ID'.nyc_taxi_warehouse.dim_datetime` d 
ON f.datetime_key = d.datetime_key
GROUP BY d.year, d.month
ORDER BY d.year, d.month'

# Or use BigQuery Console
# https://console.cloud.google.com/bigquery
```

---

## ğŸ—ï¸ Architecture

### High-Level Architecture

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                     LOCAL MODE (Development)                     â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚  Docker Compose                                                  â”‚
â”‚  â”œâ”€â”€ PostgreSQL (simulates BigQuery)                            â”‚
â”‚  â”œâ”€â”€ MinIO (object storage)                                      â”‚
â”‚  â””â”€â”€ PySpark containers                                          â”‚
â”‚                                                                   â”‚
â”‚  Storage: Local filesystem (./data/)                             â”‚
â”‚  Cost: $0                                                        â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                              â¬‡ï¸ Deploy
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                      GCP MODE (Production)                       â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”‚
â”‚  â”‚  VPC Network (10.0.0.0/16)                              â”‚   â”‚
â”‚  â”‚  â”œâ”€â”€ Private Subnet (10.0.1.0/24)                       â”‚   â”‚
â”‚  â”‚  â”‚   â””â”€â”€ Cloud Functions                              â”‚   â”‚
â”‚  â”‚  â”œâ”€â”€ Private Subnet (10.0.2.0/24)                       â”‚   â”‚
â”‚  â”‚  â”‚   â””â”€â”€ Dataproc Cluster (for Spark jobs)             â”‚   â”‚
â”‚  â”‚  â””â”€â”€ Cloud NAT (for internet access)                    â”‚   â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â”‚
â”‚                                                                   â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”‚
â”‚  â”‚  Cloud Storage (Data Lake)                              â”‚   â”‚
â”‚  â”‚  â”œâ”€â”€ gs://bronze/   - Raw data (versioned)             â”‚   â”‚
â”‚  â”‚  â”œâ”€â”€ gs://silver/   - Cleaned data                      â”‚   â”‚
â”‚  â”‚  â””â”€â”€ gs://gold/     - Dimensional model                 â”‚   â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â”‚
â”‚                                                                   â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”‚
â”‚  â”‚  BigQuery (Data Warehouse)                              â”‚   â”‚
â”‚  â”‚  â”œâ”€â”€ fact_trips (partitioned by month)                  â”‚   â”‚
â”‚  â”‚  â”œâ”€â”€ dim_datetime                                        â”‚   â”‚
â”‚  â”‚  â”œâ”€â”€ dim_location                                        â”‚   â”‚
â”‚  â”‚  â””â”€â”€ dim_payment                                         â”‚   â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â”‚
â”‚                                                                   â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”‚
â”‚  â”‚  IAM & Security                                          â”‚   â”‚
â”‚  â”‚  â”œâ”€â”€ pipeline-sa (Composer/Dataproc)                    â”‚   â”‚
â”‚  â”‚  â”œâ”€â”€ storage-sa (GCS access)                            â”‚   â”‚
â”‚  â”‚  â””â”€â”€ bq-loader-sa (BigQuery write)                      â”‚   â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### Data Flow

```
NYC TLC API
    â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ PySpark Job 1: Bronze         â”‚  Downloads raw data
â”‚ - Schema validation           â”‚  Adds metadata
â”‚ - Partitioning (year/month)   â”‚  Writes to storage
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
    â†“
GCS Bronze / Local Filesystem
    â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ PySpark Job 2: Gold           â”‚  Cleans & validates
â”‚ - Data quality checks         â”‚  Creates dimensions
â”‚ - Join zone lookup            â”‚  Deduplication
â”‚ - Dimensional modeling        â”‚  
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
    â†“
GCS Gold / Local Filesystem
    â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ PySpark Job 3: Load           â”‚  
â”‚ - BigQuery (GCP mode)         â”‚  Bulk load with WRITE_TRUNCATE
â”‚ - PostgreSQL (Local mode)     â”‚  Creates indexes
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
    â†“
BigQuery / PostgreSQL
    â†“
Analytics & BI Tools
```

### Airflow DAG

```python
# DAG: nyc_taxi_pipeline
# Schedule: Monthly (@monthly)
# Backfill: Supported with custom dates

bronze_ingestion
    â†“
gold_transformation
    â†“
data_quality_check
    â†“
bigquery_load
    â†“
analytics_report (optional)
```

---

## ğŸ“Š Data Model

### Star Schema Design

```
                    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                    â”‚  dim_datetime   â”‚
                    â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
                    â”‚ datetime_key PK â”‚
                    â”‚ pickup_datetime â”‚
            â”Œâ”€â”€â”€â”€â”€â”€â”€â”‚ year            â”‚
            â”‚       â”‚ month           â”‚
            â”‚       â”‚ day             â”‚
            â”‚       â”‚ hour            â”‚
            â”‚       â”‚ is_weekend      â”‚
            â”‚       â”‚ is_holiday      â”‚
            â”‚       â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
            â”‚
            â”‚       â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
            â”‚       â”‚  dim_location   â”‚
            â”‚       â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
            â”‚   â”Œâ”€â”€â”€â”‚ location_key PK â”‚
            â”‚   â”‚   â”‚ location_id     â”‚
            â”‚   â”‚   â”‚ borough         â”‚
            â”‚   â”‚   â”‚ zone            â”‚
            â”‚   â”‚   â”‚ service_zone    â”‚
            â”‚   â”‚   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
            â”‚   â”‚
            â†“   â†“   
    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
    â”‚     fact_trips       â”‚  â† Main fact table
    â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
    â”‚ trip_id (PK)         â”‚
    â”‚ datetime_key (FK)    â”‚â”€â”€â”€â”€â”€â”€â”€â”˜
    â”‚ pickup_location_key (FK) â”‚â”€â”€â”€â”˜
    â”‚ dropoff_location_key (FK)â”‚â”€â”€â”€â”
    â”‚ payment_key (FK)     â”‚â”€â”€â”€â”   â”‚
    â”‚ passenger_count      â”‚   â”‚   â”‚
    â”‚ trip_distance        â”‚   â”‚   â”‚
    â”‚ fare_amount          â”‚   â”‚   â”‚
    â”‚ tip_amount           â”‚   â”‚   â”‚
    â”‚ total_amount         â”‚   â”‚   â”‚
    â”‚ trip_duration_min    â”‚   â”‚   â”‚
    â”‚ data_source_month    â”‚   â”‚   â””â”€â”€â”€â”
    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â”‚       â”‚
            â†“                  â”‚       â”‚
     â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”       â”‚       â”‚
     â”‚  dim_payment    â”‚       â”‚       â”‚
     â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤       â”‚       â”‚
     â”‚ payment_key PK  â”‚â”€â”€â”€â”€â”€â”€â”€â”˜       â”‚
     â”‚ payment_type_id â”‚               â”‚
     â”‚ payment_name    â”‚               â”‚
     â”‚ is_card         â”‚               â”‚
     â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜               â”‚
                                       â”‚
                        (dropoff location)
```

### BigQuery Implementation

**Partitioning Strategy:**
- `fact_trips`: Partitioned by `data_source_month` (STRING, YYYY-MM format)
- Clustering: `datetime_key`, `pickup_location_key`

**Benefits:**
- Efficient historical queries
- Automatic partition pruning
- Cost optimization (scan only needed partitions)

**Sample BigQuery DDL:**
```sql
CREATE TABLE `project.nyc_taxi_warehouse.fact_trips`
(
  trip_id INT64,
  datetime_key INT64,
  pickup_location_key INT64,
  dropoff_location_key INT64,
  payment_key INT64,
  passenger_count INT64,
  trip_distance FLOAT64,
  fare_amount FLOAT64,
  total_amount FLOAT64,
  trip_duration_minutes FLOAT64,
  data_source_month STRING
)
PARTITION BY data_source_month
CLUSTER BY datetime_key, pickup_location_key;
```

---

## ğŸ’° Cost Management & Free Tier

### GCP Free Tier Limits

| Service | Free Tier | Expected Usage (1 month data) | Cost |
|---------|-----------|-------------------------------|------|
| **Cloud Storage** | 5 GB | ~2 GB | $0 |
| **BigQuery Storage** | 10 GB | ~1.5 GB | $0 |
| **BigQuery Queries** | 1 TB/month | ~50 GB | $0 |
| **Cloud Composer** | Not free | ~$300/month | **ğŸ’° Main cost** |
| **Dataproc** | Not free | ~$0.10/hour when running | **ğŸ’° Per-use** |
| **Networking** | 1 GB egress/month | Minimal | $0 |

### Cost Optimization Strategies

#### 1. **Use Preemptible/Spot Instances**
```hcl
# terraform/main.tf
resource "google_dataproc_cluster" "spark_cluster" {
  cluster_config {
    worker_config {
      num_instances    = 2
      preemptible      = true  # 80% cheaper
      disk_config {
        boot_disk_size_gb = 30  # Minimum
      }
    }
  }
}
```

#### 2. **Auto-Shutdown Composer When Not in Use**
```bash
# Pause Composer (saves ~$240/month when paused)
make gcp-pause-composer

# Resume when needed
make gcp-resume-composer
```

#### 3. **Scheduled Infrastructure**
```bash
# Automated teardown at night
make gcp-schedule-shutdown TIME=20:00  # 8 PM
make gcp-schedule-startup TIME=08:00   # 8 AM
```

#### 4. **Development Lifecycle**
```bash
# Work locally (free)
make local-run

# Deploy to GCP for testing
make gcp-deploy

# Run pipeline
make gcp-trigger-pipeline MONTHS=2023-01

# IMMEDIATELY destroy after testing
make gcp-destroy
```

### Monthly Cost Estimates

| Scenario | Cost | Notes |
|----------|------|-------|
| **Local only** | $0 | Perfect for development |
| **GCP - Minimal** | $150-200/month | Composer minimal config |
| **GCP - Standard** | $300-400/month | Composer + regular Dataproc |
| **GCP - With auto-pause** | $50-100/month | Composer paused nights/weekends |

### Free Tier Testing Budget

**Recommended approach for interview assignment:**

1. **Develop locally** (free): 1-2 days
2. **Deploy to GCP**: 1 day
3. **Run 1-2 test pipelines**: ~$5
4. **Record demo video**: ~$1
5. **Destroy infrastructure**: Back to $0

**Total cost for assignment: ~$6-10 using free credits**

---

## ğŸ”’ Security Implementation

### Production Security Checklist

#### âœ… Network Security
- [x] **Private VPC** with custom IP ranges
- [x] **Private IP** for Cloud Composer (no public access)
- [x] **Cloud NAT** for outbound internet (downloads)
- [x] **Firewall rules** (deny all ingress, allow specific egress)
- [x] **VPC Service Controls** (optional, for highly sensitive data)

#### âœ… IAM & Access Control
```
Service Accounts (Principle of Least Privilege):

â”œâ”€â”€ composer-sa@project.iam.gserviceaccount.com
â”‚   â”œâ”€â”€ roles/composer.worker
â”‚   â”œâ”€â”€ roles/dataproc.worker
â”‚   â””â”€â”€ roles/storage.objectViewer (GCS buckets)
â”‚
â”œâ”€â”€ dataproc-sa@project.iam.gserviceaccount.com
â”‚   â”œâ”€â”€ roles/dataproc.worker
â”‚   â”œâ”€â”€ roles/storage.objectAdmin (GCS read/write)
â”‚   â””â”€â”€ roles/bigquery.jobUser
â”‚
â””â”€â”€ bq-loader-sa@project.iam.gserviceaccount.com
    â”œâ”€â”€ roles/bigquery.dataEditor (write to tables)
    â””â”€â”€ roles/bigquery.jobUser (run load jobs)
```

#### âœ… Data Security
- [x] **Encryption at rest** (Google-managed keys)
- [x] **Encryption in transit** (TLS)
- [x] **Bucket versioning** (protect against accidental deletion)
- [x] **Lifecycle policies** (automatic data retention)
- [x] **Access logs** (Cloud Audit Logs enabled)

#### âœ… Secret Management
```bash
# Store sensitive values in Secret Manager
gcloud secrets create db-password --data-file=- <<< "your-password"

# Access in pipeline
gcloud secrets versions access latest --secret="db-password"
```

### Security Configuration

**Terraform enforces security by default:**

```hcl
# VPC with private networking
resource "google_compute_network" "vpc" {
  name                    = "nyc-taxi-vpc"
  auto_create_subnetworks = false
}

# Private subnet
resource "google_compute_subnetwork" "private" {
  name          = "private-subnet"
  ip_cidr_range = "10.0.1.0/24"
  network       = google_compute_network.vpc.id
  
  private_ip_google_access = true  # Access Google APIs privately
}

# Cloud NAT for outbound only
resource "google_compute_router_nat" "nat" {
  name   = "composer-nat"
  router = google_compute_router.router.name
  
  nat_ip_allocate_option = "AUTO_ONLY"
  source_subnetwork_ip_ranges_to_nat = "ALL_SUBNETWORKS_ALL_IP_RANGES"
}

# Firewall: Deny all ingress by default
resource "google_compute_firewall" "deny_all_ingress" {
  name    = "deny-all-ingress"
  network = google_compute_network.vpc.name
  
  deny {
    protocol = "all"
  }
  
  direction = "INGRESS"
  priority  = 1000
}
```

---

## ğŸ”„ Development Workflow

### Daily Development Flow

```bash
# 1. Work on local mode (no costs)
make local-up
make local-run MONTHS=2023-01

# 2. Make changes to code
vim etl/jobs/gold_transformation.py

# 3. Test locally
make test
make local-run MONTHS=2023-01

# 4. Commit changes
git add .
git commit -m "feat: improve gold transformation"
git push

# 5. CI/CD runs automatically (GitHub Actions)
# - Linting
# - Tests  
# - Terraform validation

# 6. Deploy to GCP for final testing
make gcp-deploy
make gcp-trigger-pipeline MONTHS=2023-01

# 7. Verify results
make gcp-query-bigquery

# 8. Tear down GCP (save costs)
make gcp-destroy
```

### Branch Strategy

```
main
  â”œâ”€â”€ develop (active development)
  â”‚   â”œâ”€â”€ feature/bronze-optimization
  â”‚   â”œâ”€â”€ feature/bigquery-partitioning
  â”‚   â””â”€â”€ fix/data-quality-check
  â””â”€â”€ production (deployed to GCP)
```

**Workflow:**
1. Develop on feature branches
2. Test locally with `make local-run`
3. PR to `develop` â†’ CI/CD runs
4. Merge to `main` â†’ Deploy to GCP (manual approval)

---

## ğŸ§¨ Teardown & Cleanup

### Quick Cleanup (Keep Infrastructure)

```bash
# Stop local services
make local-down

# Pause GCP resources (saves money, keeps config)
make gcp-pause
```

### Full Teardown (Complete Deletion)

#### Option 1: Terraform Destroy (Recommended)

```bash
# Destroy all GCP resources
make gcp-destroy

# Or manual:
cd terraform
terraform destroy -var-file="terraform.tfvars"
```

**What gets deleted:**
- âœ“ Cloud Composer environment
- âœ“ GCS buckets (and all data)
- âœ“ BigQuery dataset (and all tables)
- âœ“ Service accounts
- âœ“ VPC, subnets, firewall rules
- âœ“ NAT gateway

**What remains:**
- âœ“ GCP project (you can delete manually)
- âœ“ Terraform state (in GCS backend)

#### Option 2: Manual Cleanup

```bash
# Delete Composer (biggest cost)
gcloud composer environments delete nyc-taxi-pipeline --location us-central1

# Delete GCS buckets
gsutil rm -r gs://YOUR_PROJECT-nyc-taxi-bronze
gsutil rm -r gs://YOUR_PROJECT-nyc-taxi-silver  
gsutil rm -r gs://YOUR_PROJECT-nyc-taxi-gold

# Delete BigQuery dataset
bq rm -r -f -d nyc_taxi_warehouse

# Delete service accounts
gcloud iam service-accounts delete composer-sa@PROJECT.iam.gserviceaccount.com
gcloud iam service-accounts delete dataproc-sa@PROJECT.iam.gserviceaccount.com
```

#### Option 3: Nuke Entire Project

```bash
# Nuclear option - delete everything
gcloud projects delete YOUR_PROJECT_ID
```

### Verify Cleanup

```bash
# Check no resources remain
make gcp-verify-cleanup

# Should show: No active resources found
```

### Cost Verification

```bash
# Check current charges
gcloud billing accounts list
gcloud billing projects describe YOUR_PROJECT_ID

# View detailed billing
# Go to: https://console.cloud.google.com/billing
```

---

## ğŸ“ Project Structure

```
nyc-taxi-pipeline/
â”œâ”€â”€ README.md                          â† You are here
â”œâ”€â”€ Makefile                           â† All commands (local + GCP)
â”œâ”€â”€ docker-compose.yml                 â† Local development services
â”œâ”€â”€ .env.example                       â† Environment template
â”‚
â”œâ”€â”€ pyspark/                           â† Pipeline code
â”‚   â”œâ”€â”€ jobs/
â”‚   â”‚   â”œâ”€â”€ bronze_ingestion.py       â† Job 1: Ingest raw data
â”‚   â”‚   â”œâ”€â”€ gold_transformation.py    â† Job 2: Transform to dimensional model
â”‚   â”‚   â”œâ”€â”€ bigquery_loader.py        â† Job 3a: Load to BigQuery (GCP)
â”‚   â”‚   â””â”€â”€ postgres_loader.py        â† Job 3b: Load to PostgreSQL (local)
â”‚   â”œâ”€â”€ common/
â”‚   â”‚   â”œâ”€â”€ config.py                 â† Dual-mode configuration
â”‚   â”‚   â”œâ”€â”€ spark_session.py          â† Spark factory (local vs GCP)
â”‚   â”‚   â”œâ”€â”€ gcs_utils.py              â† GCS operations
â”‚   â”‚   â””â”€â”€ bq_utils.py               â† BigQuery operations
â”‚   â””â”€â”€ requirements.txt
â”‚
â”‚
â”œâ”€â”€ terraform/                         â† Infrastructure as Code
â”‚   â”œâ”€â”€ main.tf                       â† Main config
â”‚   â”œâ”€â”€ vpc.tf                        â† VPC, subnets, NAT
â”‚   â”œâ”€â”€ gcs.tf                        â† Storage buckets
â”‚   â”œâ”€â”€ bigquery.tf                   â† BigQuery dataset + tables
â”‚   â”œâ”€â”€ composer.tf                   â† Cloud Composer (Airflow)
â”‚   â”œâ”€â”€ iam.tf                        â† Service accounts + roles
â”‚   â”œâ”€â”€ security.tf                   â† Firewall rules
â”‚   â”œâ”€â”€ variables.tf                  â† Input variables
â”‚   â”œâ”€â”€ outputs.tf                    â† Output values
â”‚   â””â”€â”€ terraform.tfvars.example      â† Example config
â”‚
â”œâ”€â”€ sql/                              â† Database schemas
â”‚   â”œâ”€â”€ bigquery/
â”‚   â”‚   â”œâ”€â”€ create_tables.sql         â† BigQuery DDL
â”‚   â”‚   â””â”€â”€ sample_queries.sql        â† Analytics queries
â”‚   â””â”€â”€ postgres/
â”‚       â”œâ”€â”€ create_schema.sql         â† PostgreSQL DDL
â”‚       â””â”€â”€ sample_queries.sql        â† Analytics queries
â”‚
â”œâ”€â”€ tests/                            â† Test suite
â”‚   â”œâ”€â”€ conftest.py                   â† pytest fixtures
â”‚   â”œâ”€â”€ test_bronze_ingestion.py
â”‚   â”œâ”€â”€ test_gold_transformation.py
â”‚   â”œâ”€â”€ test_bigquery_loader.py
â”‚   â””â”€â”€ test_local_vs_gcp.py          â† Consistency tests
â”‚
â”œâ”€â”€ .github/
â”‚   â””â”€â”€ workflows/
â”‚       â”œâ”€â”€ ci.yml                    â† CI pipeline
â”‚       â””â”€â”€ deploy.yml                â† CD pipeline (GCP)
â”‚
â””â”€â”€ docs/
    â”œâ”€â”€ ARCHITECTURE.md               â† Technical deep-dive
    â”œâ”€â”€ SECURITY.md                   â† Security documentation
    â”œâ”€â”€ COST_OPTIMIZATION.md          â† Cost management guide
    â””â”€â”€ TROUBLESHOOTING.md            â† Common issues
```

---

## ğŸ¯ Makefile Commands Reference

### Local Development

```bash
make local-setup          # Install dependencies, setup environment
make local-up             # Start all local services (PostgreSQL, Airflow)
make local-down           # Stop all services (keep data)
make local-clean          # Stop and remove all data
make local-run            # Run pipeline locally (bronzeâ†’goldâ†’postgres)
make local-db             # Connect to local PostgreSQL
make local-airflow-ui     # Open Airflow UI in browser
make local-logs           # Tail all service logs
```

### Testing

```bash
make test                 # Run all tests
make test-coverage        # Run tests with coverage report
make test-integration     # Run integration tests (local + GCP)
make lint                 # Run linting (black, flake8, pylint)
make format               # Auto-format code
```

### GCP Deployment

```bash
make gcp-verify           # Check GCP prerequisites
make gcp-init             # Initialize Terraform
make gcp-plan             # Preview infrastructure changes
make gcp-deploy           # Deploy all infrastructure
make gcp-destroy          # Destroy all infrastructure
make gcp-outputs          # Show Terraform outputs

make gcp-upload-dags      # Upload Airflow DAGs to Composer
make gcp-trigger-pipeline # Trigger pipeline run
make gcp-logs             # View pipeline logs
make gcp-query-bigquery   # Run sample BigQuery query
```

### Cost Management

```bash
make gcp-pause            # Pause Composer (save costs)
make gcp-resume           # Resume Composer
make gcp-cost-estimate    # Estimate current monthly cost
make gcp-verify-cleanup   # Verify no resources remain
```

### CI/CD

```bash
make ci                   # Run all CI checks (lint + test + terraform)
make ci-local             # Run CI checks locally
```

---

## ğŸ§ª Testing Strategy

### 1. Unit Tests (Local)
```bash
pytest tests/test_bronze_ingestion.py -v
```

### 2. Integration Tests (Local)
```bash
# Full pipeline on sample data
make test-integration-local
```

### 3. End-to-End Tests (GCP)
```bash
# Deploy to GCP and run pipeline
make test-integration-gcp MONTHS=2023-01
```

### 4. Data Quality Tests
```bash
# Validate data quality
make test-data-quality
```

---

## ğŸ› Troubleshooting

### Local Issues

**PostgreSQL won't start:**
```bash
docker-compose down -v  # Remove volumes
make local-up
```

**Airflow tasks failing:**
```bash
make local-logs         # Check logs
docker-compose restart airflow-scheduler
```

**Out of memory:**
```bash
# Increase Docker memory to 8GB
# Docker Desktop â†’ Settings â†’ Resources â†’ Memory
```

### GCP Issues

**Terraform deployment fails:**
```bash
# Check quotas
gcloud compute project-info describe --project=YOUR_PROJECT

# Check APIs enabled
make gcp-verify
```

**Composer creation timeout:**
- Composer can take 20-30 minutes to create
- Check status: `gcloud composer environments list`

**BigQuery load fails:**
```bash
# Check service account permissions
gcloud projects get-iam-policy YOUR_PROJECT

# Check BigQuery quotas
bq show --project_id=YOUR_PROJECT
```

**Cost spike:**
```bash
# Immediately pause Composer
make gcp-pause

# Check what's running
gcloud compute instances list
gcloud dataproc clusters list

# Destroy if needed
make gcp-destroy
```

---

## ğŸ“š Additional Documentation

- [Architecture Deep Dive](docs/ARCHITECTURE.md)
- [Security Documentation](docs/SECURITY.md)
- [Cost Optimization Guide](docs/COST_OPTIMIZATION.md)
- [Troubleshooting Guide](docs/TROUBLESHOOTING.md)
- [API Reference](docs/API.md)

---

## ğŸ“ Learning Resources

- [NYC TLC Data Documentation](https://www.nyc.gov/site/tlc/about/tlc-trip-record-data.page)
- [BigQuery Best Practices](https://cloud.google.com/bigquery/docs/best-practices)
- [Cloud Composer Documentation](https://cloud.google.com/composer/docs)
- [Terraform GCP Provider](https://registry.terraform.io/providers/hashicorp/google/latest/docs)
- [Apache Airflow Documentation](https://airflow.apache.org/docs/)

---

## ğŸ¤ Contributing

This is an interview assignment project. For production use:
1. Fork the repository
2. Create feature branch
3. Make changes with tests
4. Submit pull request

---

## ğŸ“„ License

MIT License - See LICENSE file for details

---

## ğŸ‘¤ Author

Built for GCP Data Engineer role assessment

**Key Features Demonstrated:**
- âœ… Production-grade GCP architecture
- âœ… Dual-mode development (local + cloud)
- âœ… Cost-optimized design
- âœ… Comprehensive security
- âœ… Full CI/CD pipeline
- âœ… Extensive documentation

**Ready for submission!** ğŸš€
